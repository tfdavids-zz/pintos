       	+-------------------------+
		     | CS 140                  |
		     | PROJECT 4: FILE SYSTEMS |
		     | DESIGN DOCUMENT         |
		     +-------------------------+

---- GROUP ----

>> Fill in the names and email addresses of your group members.

Thomas Davids <tdavids@stanford.edu>
Akshay Agrawal <akshayka@stanford.edu>
Robert Gasparyan <robertga@stanford.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

		     INDEXED AND EXTENSIBLE FILES
		     ============================

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.
/* On-disk inode.
   Must be exactly BLOCK_SECTOR_SIZE bytes long. */
struct inode_disk
  {
    block_sector_t block_ptrs[PTRS_PER_INODE];     /* Block ptrs in inode */
    off_t length;                                  /* File size in bytes. */
    unsigned magic;                                /* Magic number. */
    uint32_t sectors;                              /* Number of data sectors */
    uint32_t type;                                 /* I_FILE or I_DIR */
    uint32_t unused[110];                          /* Not used. */
  };

/* A block of zeros, used for initalizing memory */
static char zeros[BLOCK_SECTOR_SIZE];

/* Indirect and doubly indirect blocks on disk */
struct indir_block_disk
  {
    /* Block ptrs in indir block */ 
    block_sector_t block_ptrs[PTRS_PER_INDIR_BLOCK];   
  };

>> A2: What is the maximum size of a file supported by your inode
>> structure?  Show your work.

We use 12 direct, 1 indirect and 1 doubly indirect block per inode.
Each indirect block points to 128 dir blocks, and each doubly indirect 
block points to 128 indirect blocks;
The maximum size is 

12 * 512bytes + 1*128*512bytes + 1*128*128*512bytes = 8460288 bytes = 
= 8.068 MBytes
 
---- SYNCHRONIZATION ----

>> A3: Explain how your code avoids a race if two processes attempt to
>> extend a file at the same time.
We use a per inode lock to synchronize extending of a file. So one process
will have to wait for the other to finish growing the file, write the new
length to disk and release the lock. At this point the new process will attempt
to extend the file to a desired size but if the file has already been extended
to that size by the previous process we just return true.
Key observation is that processes specify what size they wish the file to be
extended to, not by how much.

>> A4: Suppose processes A and B both have file F open, both
>> positioned at end-of-file.  If A reads and B writes F at the same
>> time, A may read all, part, or none of what B writes.  However, A
>> may not read data other than what B writes, e.g. if B writes
>> nonzero data, A is not allowed to see all zeros.  Explain how your
>> code avoids this race.

This is prevented by only updating the length of the file after all the data
has been successfully written into the newly expanded blocks. Since readers
never read beyond the current file length they will never see zeroed blocks or
half written data.

>> A5: Explain how your synchronization design provides "fairness".
>> File access is "fair" if readers cannot indefinitely block writers
>> or vice versa.  That is, many processes reading from a file cannot
>> prevent forever another process from writing the file, and many
>> processes writing to a file cannot prevent another process forever
>> from reading the file.

In our implementation reads do not block on writes. And writes block only on
other writes. So neither readers can block writers indefinitly nor visa versa.

---- RATIONALE ----

>> A6: Is your inode structure a multilevel index?  If so, why did you
>> choose this particular combination of direct, indirect, and doubly
>> indirect blocks?  If not, why did you choose an alternative inode
>> structure, and what advantages and disadvantages does your
>> structure have, compared to a multilevel index?

Yes, we used a multilevel index, with 12 direct blocks an indirect block, and a
doubly indirect block. This approach (which was discussed in class) allows us
to keep smaller files just in direct blocks, but still allows files to grow to
much larger than the total file system partition space.

			    SUBDIRECTORIES
			    ==============

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

In fdtable.c:

/* The fd_type is used to distnguish between files and directories
   in the file descriptor table. */
enum fd_type
  {
    FD_DIRECTORY,
    FD_FILE,
  };

/* The fd_entry is used as a wrapper around a file or directory
   in the file descriptor table. */
struct fd_entry
  {
    enum fd_type fd_type;

    /* An fd_entry can hold either a file or
       a directory, depending on the type. */
    union
      {
        struct file *file;
        struct dir *dir;
      };
  };

In inode.c:
Within struct inode ...
    struct lock d_lock;      /* Synchronize access to underlying directory,
                                if any. */

---- ALGORITHMS ----

>> B2: Describe your code for traversing a user-specified path.  How
>> do traversals of absolute and relative paths differ?

We use the method dir_resolve_path to traverse a user-specified path and locate
the given file or directory. This method takes in a string indicating a path
and a pointer to a struct dir *, which is where the bottom-most directory will
be stored. If there is a leading slash (indicating an absolute path), or if the
working directory is NULL, we open the root directory and start from there.
Otherwise, we reopen the current working directory and start from there. In
both cases, we jump through the directories in order until we find the final
one, then fill the char name[] with the filename. We also ensure that each
named directory in the path is indeed a directory, and fail if that is
not the case.

---- SYNCHRONIZATION ----

>> B4: How do you prevent races on directory entries?  For example,
>> only one of two simultaneous attempts to remove a single file
>> should succeed, as should only one of two simultaneous attempts to
>> create a file with the same name, and so on.

Each inode carries a struct lock d_lock, meant to synchronize access
to the underlying directory (that is, if the inode represents a
directory). The inode interface offers wrapper functions that allow
the directory code to lock and unlock the directory lock. The directory
code elects to do so when creating and removing files, adding
directory entries, and when looking up directories by their sector number,
in order to avoid races.

This design ensures that different directories can be accessed concurrently,
at the expense of making operations on a given directory sequential.

>> B5: Does your implementation allow a directory to be removed if it
>> is open by a process or if it is in use as a process's current
>> working directory?  If so, what happens to that process's future
>> file system operations?  If not, how do you prevent it?

We do not allow a directory to be removed if it is open by a process, or if it
is in use as a process's current working directory. To prevent a process
from from removing such a directory, we only a directory to be removed
if its inode open count is 0. Note that, in our design, a process always
has its current working directory open (unless if that directory is root,
in which case the directory cannot be removed anyway). Thus, we do not
need to treat process' working directory as a special case when attempting
removal.

---- RATIONALE ----

>> B6: Explain why you chose to represent the current directory of a
>> process the way you did.

We chose to represent the current directory like this because it simplifies the
logic for removing directories (discussed in B5). While it might be simpler or
more efficient to just store the inumber of the current working directory, our
approach makes it easier to check whether we can remove a directory. If we only
stored the inumber, then, upon directory removal, we would have to exhaustively
examine threads and check that their working inumber did not equal the inumber
of the directory being removed.

			     BUFFER CACHE
			     ============

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

/* reader-writer locks for controlling access to cache and cache blocks */
struct rw
{
  int num_readers, num_writers, num_waiting_writers;
  struct lock l;
  struct condition can_read, can_write;
};

/* struct identifying a member of the cache (we keep at most 64 of these) */
struct cache_entry
{
  struct list_elem elem;   /* For the cache list. */
  struct list_elem d_elem; /* For the dirty list. */

  struct block *block;
  block_sector_t sector;
  bool accessed;                /* True if entry has been used recently. */
  bool loading;                 /* True if entry's data is being loaded. */
  bool dirty;                   /* True if entry's data has been modified. */
  bool writing_dirty;           /* True if writing to disk. */
  bool should_read_ahead;       /* True if should be read ahead. */
  char data[BLOCK_SECTOR_SIZE]; /* The cached data. */
  struct rw l;                  /* To synchronize access to the entry. */
};

---- ALGORITHMS ----

>> C2: Describe how your cache replacement algorithm chooses a cache
>> block to evict.

We use a version of the clock (second-chance) algorithm, in which we scan
through the cache to see which blocks haven't recently been accessed. If we
come across a dirty block, we start writing it to disk and keep scanning. Also,
if we encounter a block which is waiting on I/O (for example, if it's being
loaded from disk), we skip it and let it finish loading.

>> C3: Describe your implementation of write-behind.

When we write a block, we don't immediately write to disk, but instead just set
the "dirty" bit on the cache entry. When we come across the block when trying
to evict from the cache, we spawn off a new thread which starts writing the
block to disk. This thread resets the dirty bit once it's done, and the block
can safely be evicted. We also write periodically (every 30 seconds), and in
the function cache_flush, which is called when the cache is destroyed.

>> C4: Describe your implementation of read-ahead.

When a block is read, we automatically insert a cache_entry for the next block
as well. This cache entry has a bit set indicating that it hasn't been loaded
yet; to remedy this, another thread (created at initialization) is constantly
reading these blocks from disk. It waits on a condition to make sure there are
blocks to be read, but then fills them in the background.

---- SYNCHRONIZATION ----

>> C5: When one process is actively reading or writing data in a
>> buffer cache block, how are other processes prevented from evicting
>> that block?

Each block has a reader-writer lock which must be held in order to evict the
block. When a process reads or writes data, it must hold the lock, so another
process trying to evict the block would have to wait on the lock, which forces
it to wait until the first process is done with reading or writing.

>> C6: During the eviction of a block from the cache, how are other
>> processes prevented from attempting to access the block?

We also include a coarse reader-writer lock for the entire cache. "Reading"
this lock means that a process is accessing blocks, and the "writer" portion
must be held in order to evict blocks. This has the effect that no process can
evict blocks while another process is reading them.

---- RATIONALE ----

>> C7: Describe a file workload likely to benefit from buffer caching,
>> and workloads likely to benefit from read-ahead and write-behind.

A file workload which repeatedly accessed some small (<50) set of files would
benefit from buffer caching, since it could read and write most of the data
from the cache, instead of always going to disk. A file workload which accessed
files sequentially (one block at a time) would benefit from read-ahead, since
the next block of the file would be automatically loaded every time we read one
block. A file workload which performed many more writes than reads would
benefit from write-behind, since it would be spared the necessity of writing to
disk on each of these writes.

			   SURVEY QUESTIONS
			   ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students in future quarters?

>> Any other comments?
