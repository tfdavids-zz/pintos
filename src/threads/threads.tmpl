			+--------------------+
			| CS 140             |
			| PROJECT 1: THREADS |
			| DESIGN DOCUMENT    |
			+--------------------+
				   
---- GROUP ----

Thomas Davids <tdavids@stanford.edu>
Akshay Agrawal <akshayka@stanford.edu>
Robert Gasparyan <robertga@stanford.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

			     ALARM CLOCK
			     ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

/* Ordered list to keep track of sleeping threads so we know who we might have
 * to wake up.
 */
static struct list sleep_list;

Within struct thread:

int64_t wakeup_time; // Time that this thread is supposed to wake up (assuming
                     // it's asleep).
struct list_elem sleepelem; // List element for inserting this thread into
                            // sleep_list.

---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

When timer_sleep() is called, the wakeup time of the current thread is updated
to the correct value and the thread is inserted into sleep_list at the correct
location (so we maintain an ordering of the list where the first element is
the first to be woken).

// TODO: timer interrupt handler?

>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

// TODO

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

Timer interrupts are disabled when timer_sleep() is called, so only one thread
can be inserted to the sleep_list at once.

// TODO: do interrupts ever get re-enabled? Should we switch lines 119 and 120?

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

Since interrupts are disabled, we can't have a timer interrupt during this
call; the interruptor will have to wait until the thread has been put to sleep
before acting.

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

// TODO

			 PRIORITY SCHEDULING
			 ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

Within struct lock:

struct list_elem elem;      /* List element for locks held by thread. */
int priority;               /* Priority of highest thread that wants the
                             * lock. */

Within struct thread:

int eff_priority;                   /* Effective priority of this thread
                                     * (including donation). */
struct list lock_list;              /* List of all locks held by this thread. */
struct lock *blocking_lock;         /* The lock this thread is blocked on */

In thread.c:

static struct list ready_lists[NUM_PRIO]; /* Ready lists of threads, sorted
                                           * by effective priority. */

>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

Essentially, each thread maintains a value for priority, which represents its
assigned priority, and effective priority, which represents the priority that
it acts with, including donation. In addition to effective priority, each
thread keeps track of which locks it's holding, and which one (if any) that
it's blocked on. This lets a thread that tries to acquire a lock donate its
priority "up the chain" very efficiently.

 ---------------------    -------------------    ---------------------
| Thread A            |  | Thread B          |  | Thread C            |
| lock_list: none     |  | lock_list: L1     |  | lock_list: L2       |
| prio: 8             |  | prio: 5           |  | prio: 3             |
| eff_prio: 8         |  | eff_prio: 5       |->| eff_prio: 5         |
| blocking_lock: none |  | blocking_lock: L2 |  | blocking_lock: none |
 ---------------------    -------------------    ---------------------

In the current situation, we have three threads and two locks. Thread B is
waiting on lock L2 held by thread C, and thus has donated its priority 5 to
thread C. If thread A tries to acquire L1 (a lock held by thread B), lock L1
will increase its priority to 8. Since L1 knows that it's being held by thread
B, it will then give this priority of 8 to thread B as effective priority. When
thread B's priority increases, it passes this on to the holder of the lock it's
waiting on, and this process will continue until we reach a thread which is
waiting on no one. Our result will be as follows:

 -------------------    -------------------    ---------------------
| Thread A          |  | Thread B          |  | Thread C            |
| lock_list: none   |  | lock_list: L1     |  | lock_list: L2       |
| prio: 8           |  | prio: 5           |  | prio: 3             |
| eff_prio: 8       |->| eff_prio: 8       |->| eff_prio: 8         |
| blocking_lock: L1 |  | blocking_lock: L2 |  | blocking_lock: none |
 -------------------    -------------------    ---------------------

All the threads now have effective priority 8, since A has donated its priority
to each of them.

---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

When it comes time to wake up a thread waiting on a lock, semaphore, or
condition variable, we search the list of waiters (with the function list_max)
to find the one with the highest effective priority.

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

Suppose we have the situation from B2, where thread A tries to acquire a lock
held by thread B. When thread A calls lock_acquire() and sees that its lock is
already held, we enter a loop where priority is donated (as effective
priority). In this case, thread A would give its priority of 8 to the lock L1
held by thread B and to thread B. If thread B is blocked on lock L2 held by
thread C, we would repeat the process; thread B would give this priority of 8
to lock L2 and to thread C.

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

When lock_release() is called, the thread releasing the lock needs to
recalculate its priority, since its effective priority may decrease. Since
another thread could be donating priority to the releasing thread, we can't
simply reset effective priority to priority; we need to look at all the locks
we hold and see what their priority is. The releasing lock sets its priority
to the max of those, and then yields if it no longer has the highest priority.

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

thread_set_priority() sets the priority of the current thread, which updates
its effective priority as well. However, if another thread steps in at that
exact moment to donate priority to the current thread, one of those updates to
effective priority could be overwritten. This is avoided by disabling
interrupts in thread_calculate_priority(). This can't be avoided with a lock,
since a timer interrupt (e.g. a higher-priority thread waking up) could
pre-empt this thread and mess with its priority.

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

We chose this design because it only adds a constant size to each thread and
each lock. One of the other designs we considered was letting each thread
maintain a sorted array of the threads waiting on it; effective priority would
then be the priority of the first thread in this array. This would have
increased efficiency in calculating priority after releasing a lock; however,
it would have meant that the size of a thread could grow without bound. Our
implementation lets us bound the size of a thread, which keeps us safely under
the 4KB limit.

			  ADVANCED SCHEDULER
			  ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

Within struct threads:

int nice;

fixed_point_t recent_cpu;

"int nice" stores the niceness value the thread
"fixed_point_t recent_cpu" stores the recent cpu usage of the thread

Global variable(thread.c):

static fixed_point_t load_avg;

"static fixed_point_t load_avg" stores the load average of the system

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

 
timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0     0   0   0   63  61  59	A
 4     4   0   0   62  61  59	A      
 8     8   0   0   61  61  59	B
12     8   4   0   61  60  59	A
16     12  4   0   60  60  59	B
20     12  8   0   60  59  59	A	   
24     16  8   0   59  59  59	C
28     16  8   4   59  59  58	B
32     16  12  4   59  58  58	A
36     20  12  4   58  58  58	C



>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

One ambiguity we noticed when filling out the table was about how we 
treat a running thread that was just  which When filling out the table The only possible ambiguity seemed to be with choosing 

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?



---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

			   SURVEY QUESTIONS
			   ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

>> Any other comments?
